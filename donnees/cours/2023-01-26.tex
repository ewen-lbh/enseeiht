\documentclass{article}
\input{/home/ewen/enseeiht/.tooling/template.tex}
\renewcommand{\overline}{\bar}

\title{Analyse de données}
\author{Javier Cuadrado \verb|<javiercuadrado@cnrs.fr>| \and Ritvikmath}
\date{2023-01-26}

\begin{document}
	\maketitle

	\begin{asbtract}
		On note $x$ les scalaires, $\bar{x}$ les vecteurs et $\bar{\bar{x}}$ les matrices.
	\end{asbtract}

	\section{Visualisation}

	\subsection{Le problème de la projection}

	\section{ACP des individus}

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[scale=0.5]
			\draw[->] (-10, 0) -- (10, 0);
			\draw[->] (0, -5) -- (0, 10);
			\draw (4, 8) node{$ \times $};
			\draw (5, 9) node{$ \times $};
			\draw (3, 6) node{$ \times $};
			\draw (4, 2) node{$ \times $};
			\draw (5, 3) node{$ \times $};
		\end{tikzpicture}
		\caption{}
		\label{fig:}
	\end{figure}

	On prend $\{x_n, n \in \llbracket 1, N \rrbracket\} \in \R^{\llbracket 1, N \rrbracket}, N \in \N $

	On cherche à avoir la projection gardant le plus d'informations
	
	\begin{align*}
		\operatorname{Pro}_{S \bar{u}_i} (\bar{x}_i) &:= (\bar{u}^\top \bar{x}) \bar{u}_i \\
		\text{moyenne} &:= (\bar{u}_i^\top \left<\bar{x} \right>) \bar{u}_i
	\end{align*}

	La variance de la projection est 
	\begin{align*}
		\frac{1}{N} \sum_{n=1}^{N} (\bar{u}_i^\top \bar{x}_n - \bar{u}_i^\top \left<\bar{x}_i \right>)^2 &= \frac{1}{N} \sum_{n=1}^{N} \bar{u}_i^\top (\bar{x}_n - \left<\bar{x} \right>)^2 \bar{u}_i  \\
																     &= \bar{u}_i^\top \left( \underbrace{\frac{1}{N} \sum_{n=1}^{N} (\bar{x}_n - \left<\bar{x} \right>)^2}_{\bar{\bar{\Sigma}}} \right) \bar{u}_i \\
	\end{align*}

	\paragraph{Maximisation de $\boldsymbol{\bar{u}_i \bar{\bar{\Sigma}} \bar{u}_i}$}

	On prend $\lambda \in \R$ et on utilise Lagrange (cf 2e année lol)

	\begin{align*}
		\bar{u}_i^\ast &= \max_{\bar{u}_i} \bar{u}_i^\top \bar{\bar{\Sigma}} \bar{u}_i + \lambda (1 - \bar{u}_i^\top \bar{u}_i) \\
		\iff 2 \bar{\bar{\Sigma}} \bar{u}_i^\ast  - \lambda 2 \bar{u}_i^\ast &= 0 \quad&\text{en dérivant selon $\overline{u}_i$} \\
		\iff \bar{\bar{\Sigma}} \bar{u}_i^\ast &= \lambda \bar{u}_i^\ast \\
	\end{align*}


	On obtient une \emph{équation aux vecteurs propres}.

	On centre et on réduit les données:

	\begin{align*}
		x &\leftarrow \frac{x - \left<x \right>}{\sqrt{\sigma^2} }
	\end{align*}

	\begin{warning}
		Réduire les données peut être dangereux, car ça donne la même importance à toutes les variables

		\begin{itemize}
			\item Obligatoire si les variables ont des unités différentes
			\item Sinon, pas forcément pertinent
		\end{itemize}
	\end{warning}

	\subsection{Exercice 1}

	\begin{table}[H]
		\centering
		\caption{Données}
		\label{tab:donnes-exercice-1-individuel}
		\begin{tabular}{c|ccc}
		 & $v_1$ & $v_2$ & $v_3$ \\\hline
			$x_1$ & 1 & 0 & -1 \\
			$x_2$ & 2 & 1 & -3 \\
			$x_3$ & -1 & -2 & 3 \\
			$x_4$ & 0 & -1 & 1 \\
			$x_5$ & -1 & 2 & -1 \\
			$x_6$ & -2 & 1 & 1 \\
			$x_7$ & 1 & 0 & -1 \\
			$x_8$  & 0 & 1 & 1
		\end{tabular}
	\end{table}
	

	\begin{align*}
		\left<\overline{x} \right> &= \frac{1}{N} \sum_{i=1}^{N} \overline{x}_i \\
		&= \frac{1}{8} \sum_{i=1}^{N} \overline{x}_i \\
		&= \begin{pmatrix} 2 & 3 & 4 \end{pmatrix}  \\
	\end{align*}

	\begin{align*}
		\Sigma &:\ \sigma_{XY} = (X - E(X))^\top (Y - E(Y))
	\end{align*}

	\begin{align*}
		\Sigma &= \frac{1}{N} \sum_{i=1}^{N} \overline{Y}_i^\top \overline{Y}_i \\
		&= \frac{1}{N} \overline{Y}^\top \overline{Y} \\
		&= \begin{pmatrix} 1 & 0 & -1 \\ 0 & 1 & 1 \\ -1 & -1 & 2 \end{pmatrix}  \\
	\end{align*}

	Avec $Y = \begin{pmatrix} & & \\ & \text{(les données)} & \\ & &  \end{pmatrix} $

	\paragraph{}
	
	On trouve les éléments propres par un polynôme caractéristique
	\begin{align*}
		\chi_{\Sigma} &= 0 - \left( \frac{3}{2} \right)^3 \begin{vmatrix} 1 - \lambda' & 0 & -1 \\ 0 & 1-\lambda' & -1 \\ -1 & -1 & 2 - \lambda' \end{vmatrix} \quad&\text{avec $\lambda' = \frac{2}{3} \lambda$ } \\
		&= 0 \\
	\end{align*}

	On en déduit $\Sp(\Sigma) = \frac{1}{2} \{9, 3, 0\} $

	Puis $\begin{cases}
		\overline{u}_1 = \frac{1}{\sqrt{6} } \begin{pmatrix} -1 & -1 & 2 \end{pmatrix} \\
		\overline{u}_2 = \frac{1}{\sqrt{2} } \begin{pmatrix} -1 & 1 & 0 \end{pmatrix} 	\\
		\overline{u}_3 = \frac{1}{\sqrt{3} } \begin{pmatrix} -1 & -1 & -1 \end{pmatrix} 
	\end{cases}$

	On a $\overline{Y}_2 = \begin{pmatrix} 2 & 1 & -3 \end{pmatrix} $ 
	$\overline{u}_2 = \frac{1}{\sqrt{6} } \begin{pmatrix} -1 & -1 & 2 \end{pmatrix} $

	Donc $\operatorname{CP}_i |_{X_2} = \overline{y}_2'  \cdot \overline{u}_1 = -3.674 $

	\[
		\operatorname{CT}_i |_{X_n} = \frac{-(\operatorname{CP}_i |_{X_n})^2}{\sum_{i=1}^{N} (CP_i|_{X_n})}
	\] 



\end{document}
