\documentclass{article}
\input{/home/ewen/enseeiht/.tooling/template.tex}

\title{Calcul Scientifique}

\begin{document}

\maketitle
\section{Localisation des valeurs propres}
\begin{theorem}[d'Hadamard-Gerchgörin]
	Soit $A \in \cM_n(\C)$.

	On a \[
		\Sp(A) \subset \bigcup_{i=1}^n \left\{z\in \C,\ |z - a_{ii}| \le \sum_{k=1,\ k\neq i}^{n} |a_{ij}| \right\} 
	\] 

	On a donc en particulier

	\[
		\rho(A) \le \max_{i} \sum_{j=1}^{n} |a_{ij}|
	\] 

	Avec $\rho$ le  \emph{rayon spectral}, $\rho(A) = \max |\Sp(A)|$
\end{theorem}

\begin{proof}
	On a $Au = \lambda u$ avec $\lambda\in \Sp(A)$ et $u \in E_{\lambda}(A)$.

	On prend $i$ tel que $\|u\|_{\infty} = |u_i|$

	On pose $v_j = \displaystyle\frac{u_j}{u_i}$

	Donc $v_i = 1$ et $|v_j| \le 1$

	On a 
	\begin{align*}
		Av = \lambda v &\implies \sum_{j=1}^{n} a_{ij} v_j = \lambda v_i = \lambda \\
			       &\implies \lambda - a_{ii} = \sum_{j\neq i} a_{ij} v_j \\
			       &\implies |\lambda - a_{ii}| \le \sum_{j\neq i} |a_{ij} v_j| \le \sum_{j\neq i} |a_{ij}| 
	\end{align*}
\end{proof}

\begin{theorem}[Puissance itérée]
	Soit $A$ tel que $Sp(A) \subset \R^\ast$ et $\card |Sp(A)| = \card \Sp(A)$ \footnote{les valeurs propres sont distinctes en module}
	On note $(\lambda_i)_i = \Sp(A)$ par ordre de module croissant.

	Le résultat de l'algorithme suivant est $(\lambda_1, v_1)$ avec  $v_1 \in E_{\lambda_1}(A)$.

	\begin{algorithmic}
		% TODO slide page 8
	\end{algorithmic}
\end{theorem}

\begin{proof}
	\begin{align*}
		x^{(1)} &= \frac{Ax^{(0)}}{\|A x^{(0)}\|} \\
	\end{align*}

	Décomposons $x^{(0)}$ dans la base des vecteurs propres de $A$ ($A$ diagonalisable).

	On a \[
		x^{(0)} = \sum_{i=1}^{n} \xi_i u_i
	\]

	Avec $(u_i)_i$ les vecteurs propres.

	
	\begin{align*}
		x^{(1)} &= \frac{Ax^{(0)}}{\|A x^{(0)}\|} \\
		&= \frac{\sum_{i=1}^{n} \xi_i \lambda_i u_i}{\| A x^{(0)} \|} \quad&\text{car $u_i \in E_{\lambda_i}(A)$} \\
		&=: \alpha_1 \sum_{i=1}^{n} \lambda_i \xi_i u_i \\
		x^{(2)} &= \frac{Ax^{(1)}}{\|Ax^{(1)}\|} \\
		&= \frac{\alpha_1}{\|A x^{(1)}\|} \sum_{i=1}^{n} \lambda_i^2 \xi_i u_i \\
		x^{(p)} &= \alpha_p \sum_{i=1}^{n} \lambda_i^p \xi_i u_i \quad&\text{par récurrence immédiate} \\
		&= \alpha_p \lambda_1^p \left(\xi_1 u_1 + \left( \frac{\lambda_2}{\lambda_1} \right)^{p} \xi_2 u_2 + \cdots + \xi_1 \left( \frac{\lambda_n}{\lambda_1} \right)^p u_n \right) \\
		&\equivalent{p\to \infty} \alpha_p \lambda_1^p \xi_1 u_1 \quad&\text{car $\left| \frac{\lambda_j}{\lambda_1}\right| \xrightarrow[p \to \infty]{} 0$}
	\end{align*}

	Or $\forall p, \|x^{(p)}\| = 1$ donc $|\alpha_p \lambda_1^p \xi_1| \xrightarrow[p\to \infty]{} 1$.

	Et ainsi
	\[
		x^{(p)} \equivalent{p\to \infty} \frac{\alpha_p \lambda_1^p \xi_1}{|\alpha_p \lambda_1^p \xi_1|} u_1
	\] 

	\paragraph{Critère de stabilité}
	Si $p\in 2\N$ et $\lambda_1\in \R$

	\paragraph{Critère d'arrêt}
	On regarde quand \[
		\frac{\| Ax^{(p)} - \beta^{(p)} x^{(p)} \|}{|\beta^{(p)}|} < \epsilon
	\] 
\end{proof}

\subsubsection{Opération de déflation}

Pour une matrice symétrique.

\paragraph{Principes}

Soi $B = A - \lambda_1 W_1 W_1^\top$

\begin{itemize}
	\item $\rg B = n - 1$ 
	% TODO slide page ?
\end{itemize}

À chaque postmultiplication par $W_1$, on passe une valeur propre à 0 ($\lambda_1$ puis $\lambda_2$, etc.).

\paragraph{Exercice}

\emph{Soit $A\in \cM_n(\R)$. $\lambda\in \Sp A$ et $u\in E_{\lambda}(A)$. Soit $\alpha\in \R \setminus \Sp A$. Montrer que $\mu = \frac{1}{\lambda-\alpha} \in \Sp(A-\alpha I)^{-1}$, et que $u\in E_{\mu}((A-\alpha I)^{-1})$}.

On a 
\begin{align*}
	(A - \alpha I) u &= (\lambda - \alpha) u \\
\end{align*}

On a \begin{align*}
	\alpha \not\in \Sp(A) &\iff \ker (A - \alpha I) = \{0\} \\
			      &\iff A - \alpha I \in GL_n(\R)
\end{align*}

Donc \begin{align*}
	\frac{1}{1-\alpha} u &= (A - \alpha I)^{-1} u \\
\end{align*}

\emph{Soit $A \in S_n(\R) \cap GL_n(\R)$ et $|\lambda_1| > |\lambda_2| > \cdots > |\lambda_n|$ les valeurs propres de $A$. Quelles valeurs l'algorithme suivant permet-il d'obtenir ?} 

\paragraph{Méthode de la puissance itérée sur shift and inverse}
\emph{On attrape le spectre par l'autre côté, c'est la méthode de la puissance itérée sur $(A - \alpha I)^{-1}$} 

\begin{enumerate}
	\item $i\leftarrow 0$
	 \item Jusqu'à convergence 
		 \begin{enumerate}
		 	\item Résolution du système $A y_{i+1} = x_i$
			\item $x_{i+1} = \displaystyle\frac{y_{i-1}}{\| y_{i-1} \|}$ 
			\item $\beta_{i+1} = x_{i+1}^\top A x_{i+1}$
			\item $i \leftarrow i + 1$
		 \end{enumerate}
\end{enumerate}

On obtient la valeur de propre de $A$ la plus proche de  $\alpha$.

\begin{theorem}[Algorithme de Jacobi pour une matrice symmétrique]
	
	\paragraph{Principes}
	Procédé itératif jusqu'à convergence ($A_k \xrightarrow[k\to \infty]{} \diag(\lambda_1, \ldots, \lambda_n)$)
	\[
		\begin{cases}
			A_1 &= A \\
			A_{k+1} &= \Theta_k^{-1} A_k \Theta_k \\
		\end{cases}
	\] 

	On prend $\Theta_k \in \cO_n(\R)$ 
\end{theorem}

\paragraph{Rotation de Givens}

Rotation d'un angle $\alpha$ sur deux coordonnées, les autres restent inchangées.

 \begin{align*}
	\Theta = \begin{pmatrix} 
		1 & 
		 \\ & \ddots &
		 \\ & & \cos\alpha & & & & -\sin\alpha 
		 \\ & & & 1 
		 \\ & & & & \ddots &
		 \\ & & & & & 1 
		 \\ & & \sin\alpha & & & & \cos \alpha 
		 \\ & & & & & & & 1
		 \\ & & & & & & & & \ddots
		 \\ & & & & & & & & & 1
	\end{pmatrix}  
	\begin{array}
	\\
	\\
	\\
	i \\
		\\
		\\
		\\
		j \\
		  \\
		  \\
		  \\
	\end{array}
\end{align*}

\paragraph{Exemple}

On prend $\Theta$ la rotation de Givens sur  $(i, j)$ et  $\begin{cases}
	A &= A_0 \\
	A_{k+1} &= \Theta_k^\top A_k \Theta_k \\
\end{cases}$

Notons $E_k = e_i \sqcup e_j$. On a $\Theta_k E_k = E_k \begin{pmatrix} \cos \alpha & \sin \alpha \\ -\sin\alpha & \cos \alpha \end{pmatrix} $.

\paragraph{}

On a $S_k = \| A_k \|_\text{frob}^2 - D_k$  avec $D_k$ la partie diagonale et $\| \cdot \|_\text{frob} = \tr( \cdot ^\top  \cdot )$

\paragraph{}

$\Theta_k \in \cO_n(\R)$ donc $\|A_{k+1}\|_\text{frob}^2 = \| \Theta_k^\top A_k \Theta_k \|_\text{frob}^2 = \|A_k\|_\text{frob}^2 $

\begin{align*}
	S_{k+1} &=  \|A_{k+1}\|^2 - D_{k+1} \\\
	&= \| A_k \|^2 - D_{k+1} \\
	S_k &= \|A_k\|^2 - D_k \\
	\implies S_{k+1} - S_k &= D_k - D_{k+1} \\
	&= (a_{ii}^{(k)}^2 + a_{jj}^{(k)}^2) - (a_{ii}^{(k+1)}^2 + a_{jj}^{(k)}^2) \\
	&= \text{(c'est remplacé par des cosinus et des sinus, on simplifie\ldots)} \\
\end{align*}

\end{document}
